from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import tensorflow as tf
import numpy as np

import sys
import time

import began_model as began
from dataset import DataIterator
from dataset import CelebADataSet as DataSet

sys.path.append('../')
import image_utils as iu

results = {
    'output': './gen_img/',
    'checkpoint': './model/checkpoint',
    'model': './model/BEGAN-model.ckpt'
}

train_step = {
    'epoch': 25,
    'batch_size': 32,
}


def main():
    start_time = time.time()  # Clocking start

    # GPU configure
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True

    with tf.Session(config=config) as s:
        # BEGAN Model
        model = began.BEGAN(s)  # BEGAN

        # Initializing
        s.run(tf.global_variables_initializer())

        # Celeb-A DataSet images
        ds = DataSet(input_height=32,
                     input_width=32,
                     input_channel=3,
                     mode='w').images
        dataset_iter = DataIterator(ds, None, train_step['batch_size'],
                                    label_off=True)

        sample_x = ds[:model.batch_size]
        sample_x = np.reshape(sample_x, model.image_shape)
        sample_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)  # 32 x 128

        kt = tf.Variable(0., dtype=tf.float32)

        for epoch in range(train_step['epoch']):
            d_losses, g_losses = [], []
            for batch_images in dataset_iter.iterate():
                batch_x = tf.reshape(batch_images, model.image_shape)
                batch_z = np.random.uniform(-1., 1., [model.batch_size, model.z_dim]).astype(np.float32)  # 32 x 128

                # Update k_t
                # k_t+1 = k_t + lambda_k * (gamma * d_real - d_fake)
                kt += model.lambda_k * (model.gamma * model.d_real - model.d_fake)

                # Update D network
                _, d_loss = s.run([model.d_op, model.d_loss],
                                  feed_dict={
                                      model.x: batch_x,
                                      model.z: batch_z,
                                      model.kt: kt,
                                  })

                # Update G network
                _, g_loss = s.run([model.g_op, model.g_loss],
                                  feed_dict={
                                      model.z: batch_z,
                                      model.kt: kt,
                                  })
                # savin' losses
                d_losses.append(d_loss)
                g_losses.append(g_loss)

                # Summary
                """
                d_loss, g_loss, summary = s.run([model.d_loss, model.g_loss, model.merged],
                                                feed_dict={
                                                    model.x: batch_x,
                                                    model.z: batch_z,
                                                })
                # Summary saver
                model.writer.add_summary(summary, epoch)
                """

            # Print loss
            print("[+] Epoch %04d => " % epoch,
                  " D loss : {:.8f}".format(tf.reduce_mean(d_losses)),
                  " G loss : {:.8f}".format(tf.reduce_mean(g_losses)))

            # Training G model with sample image and noise
            samples = s.run(model.g,
                            feed_dict={
                                model.x: sample_x,
                                model.z: sample_z,
                                model.kt: kt,
                            })

            # Export image generated by model G
            sample_image_height = model.sample_size
            sample_image_width = model.sample_size
            sample_dir = results['output'] + 'train_{:03d}.png'.format(epoch)

            # Generated image save
            iu.save_images(samples,
                           size=[sample_image_height, sample_image_width],
                           image_path=sample_dir)

            # Model save
            model.saver.save(s, results['model'], global_step=epoch)

    end_time = time.time() - start_time  # Clocking end

    # Elapsed time
    print("[+] Elapsed time {:.8f}s".format(end_time))

    # Close tf.Session
    s.close()


if __name__ == '__main__':
    main()
